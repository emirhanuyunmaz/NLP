{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08a53ab5-f065-4c2f-8fce-6ee53eb3502b",
   "metadata": {},
   "source": [
    "# Veri Ön İşleme (Data Preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91970d35-47af-486d-9fcc-5a368fa6c6f3",
   "metadata": {},
   "source": [
    "### 1)Metinlerde Bulunan Fazla Boşlukların Ortadan Kaldırılması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f10ef2e-d711-4b49-9175-4056295cda0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello World 2020'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hello            World    2020\"\n",
    "cleaned_text1 = \" \".join(text.split())\n",
    "cleaned_text1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6d8df0-ac97-4379-bd9c-75852182a829",
   "metadata": {},
   "source": [
    "### 2)Büyük -> Küçük Harf Çevrimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f60cb288-082f-4fb5-9a0c-db3a0efc778a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello , world ! 30'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"HeLLo , WorLD ! 30\"\n",
    "cleaned_text2 = text.lower()\n",
    "cleaned_text2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669330ff-9724-406c-83f7-74c76effabff",
   "metadata": {},
   "source": [
    "### 3)Noktalama İşaretlerinin Kaldırılması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03fa901a-9361-470f-bc48-34b32ad7ff7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello  World 2222'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "text = \"Hello , World! 2222;;\"\n",
    "cleaned_text3 = text.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "cleaned_text3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e5298-0e9d-44db-ba24-85352c8612bf",
   "metadata": {},
   "source": [
    "### 4)Özel Karakterlerin Kaldırılması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81618b8a-54fe-47c3-a4ee-bd8444c11f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello World          '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# < > | @ \\ / % * #\n",
    "text = \"Hello World! < > | @  / % * # \"\n",
    "cleaned_text4 = re.sub(r\"[^A-Za-z0-9\\s]\",\"\",text)\n",
    "cleaned_text4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9328887a-5b50-4bda-bf3c-c8d499c1a766",
   "metadata": {},
   "source": [
    "### 5)Yazım Hatalarını Düzeltme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f8e5587-8b7b-4562-8767-b907708b1c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Hello World! 2200\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob # Metin Analizlerinde Kullanılan bir kütüphane\n",
    "text = \"Hellıo Wirld! 2200\"\n",
    "cleaned_text5 = TextBlob(text).correct() # \"correct\" yazım hatalarını düzeltir\n",
    "cleaned_text5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b88fff-ebfd-409e-9ac8-6e1f268668fd",
   "metadata": {},
   "source": [
    "### 6)HTML ya da URL Etiketlerinin Kaldırılması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f4f4838-7f6b-4150-ab64-d464929cad19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Hello World ! 2020 '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "text = \"<div> Hello World ! 2020 </div>\"\n",
    "#BeautifulSoup ile HTML yapısını parse et , get_text ile text kısmını çek\n",
    "cleaned_text6 = BeautifulSoup(text,\"html.parser\").get_text()\n",
    "cleaned_text6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8669bdd4-e05f-4bb1-afa0-8957bf53fd73",
   "metadata": {},
   "source": [
    "### 7)Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32a8e0bd-0722-4191-a1ce-6fd815018c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\emirh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk # Natural Language Toolkit\n",
    "\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "text = \"Hello , World ! How are you ? Hello , hi ...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56ca8d91-10bf-4a52-b785-fdc4b197ec16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " 'World',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Hello',\n",
       " ',',\n",
       " 'hi',\n",
       " '...']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kelime tokenizasyonu (word_tokenize) : Metni kelimelere ayırır .\n",
    "# Noktalama işaretleri ve boşluklar ayrı birer \"token\" olarak elde edilecektir .\n",
    "word_tokens = nltk.word_tokenize(text)\n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0e661e6-e07f-47a3-bda1-901725c0296b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello , World !', 'How are you ?', 'Hello , hi ...']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cümle tokenize işlemi (sent_tokenize) : Metni cümlelere ayırır . Her bir cümle birer token olur .\n",
    "sentence_tokens = nltk.sent_tokenize(text)\n",
    "sentence_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f7c130-d131-4c83-9718-319480a00a62",
   "metadata": {},
   "source": [
    "### 8)Kök ve Gövde Analizi (Stemming Lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e405390d-0cc0-43e3-848b-a9735894f011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stems:['run', 'runner', 'ran', 'run', 'better', 'go', 'went']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\emirh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Porter Stemmer nesnesi oluşturma\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = [\"running\" , \"runner\" , \"ran\" , \"runs\" , \"better\" , \"go\" , \"went\"]\n",
    "\n",
    "# Kelimelerin köklerini buluyoruz bunu yaparken PorterStemmer 'in stem() fonk. kullanıyoruz\n",
    "stems = [stemmer.stem(w) for w in words]\n",
    "print(f\"Stems:{stems}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b92051e0-3200-43d4-8266-147d50e93da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmas:['run', 'runner', 'run', 'run', 'better', 'go', 'go']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [\"running\" , \"runner\" , \"ran\" , \"runs\" , \"better\" , \"go\" , \"went\"]\n",
    "lemmas = [lemmatizer.lemmatize(w,pos = \"v\") for w in words]\n",
    "print(f\"Lemmas:{lemmas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cd432b-9db3-4026-be73-40db4ef8ffc3",
   "metadata": {},
   "source": [
    "### 9)Durdurma Kelimeleri (Stop Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "312bb74d-5958-4663-8014-add5ef3053f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\emirh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['examples', 'handling', 'stop', 'words', 'texts.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\") # Farklı dillerde en çok kullanılan stop word içeren veri seti\n",
    "#Ingilizce stopwords analizi (nltk)\n",
    "stop_words_eng = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Örnek Ingilizce metin \n",
    "text = \"There are some examples of handling stop words some texts.\"\n",
    "text_list = text.split()\n",
    "# Eğer word ingilizce stop words listesinde (stop_words_eng) yoksa , bu kelime filtrelenmiş listeye ekliyoruz. \n",
    "filtered_words = [word for word in text_list if word.lower() not in stop_words_eng]\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32a520ff-eea5-47f2-8a14-06d0236f7b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\emirh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Merhaba', 'bugün', 'hava', 'güzel']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\") # Farklı dillerde en çok kullanılan stop word içeren veri seti\n",
    "#Türkçe stopwords analizi (nltk)\n",
    "stop_words_tr = set(stopwords.words(\"turkish\"))\n",
    "\n",
    "#Örnek Türkçe Metin\n",
    "metin = \"Merhaba bugün hava çok güzel\"\n",
    "metin_list = metin.split()\n",
    "\n",
    "filtered_words_tr = [word for word in metin_list if word.lower() not in stop_words_tr]\n",
    "filtered_words_tr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58853027-fdfb-4f73-9328-a7a477330ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
